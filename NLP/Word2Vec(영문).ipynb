{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://localhost:4040\n",
       "SparkContext available as 'sc' (version = 2.4.6, master = local[*], app id = local-1606995829709)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.{CountVectorizer, IDF}\n",
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.sql.{DataFrame, Dataset, SparkSession}\n",
       "import scala.collection.mutable.ArrayBuffer\n",
       "import java.util.Properties\n",
       "import java.io.ByteArrayInputStream\n",
       "import scala.collection.JavaConversions._\n",
       "import edu.stanford.nlp.ling.CoreAnnotations.{LemmaAnnotation, SentencesAnnotation, TokensAnnotation}\n",
       "import edu.stanford.nlp.pipeline.{Annotation, StanfordCoreNLP}\n",
       "import spark.implicits._\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.{CountVectorizer, IDF}\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.{DataFrame, Dataset, SparkSession}\n",
    "import scala.collection.mutable.ArrayBuffer\n",
    "import java.util.Properties\n",
    "import java.io.ByteArrayInputStream;\n",
    "import scala.collection.JavaConversions._\n",
    "\n",
    "import edu.stanford.nlp.ling.CoreAnnotations.{LemmaAnnotation, SentencesAnnotation, TokensAnnotation}\n",
    "import edu.stanford.nlp.pipeline.{Annotation, StanfordCoreNLP}\n",
    "\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res0: org.apache.spark.SparkContext = org.apache.spark.SparkContext@7527a9b1\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "plainText: org.apache.spark.rdd.RDD[String] = Data/proj/T5.csv MapPartitionsRDD[1] at textFile at <console>:41\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val plainText = sc.textFile(\"Data/proj/T5.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stopWords: scala.collection.immutable.Set[String] = Set(down, it's, ourselves, that's, for, method, further, she'll, any, there's, this, haven't, in, ought, myself, have, your, off, once, i'll, are, is, his, why, too, why's, am, than, isn't, didn't, himself, but, you're, below, what, would, methods, i'd, if, you'll, own, they'll, up, we're, they'd, so, our, do, all, him, had, nor, before, it, a, she's, as, hadn't, because, has, she, yours, or, above, yourself, herself, she'd, such, they, each, can't, don't, i, until, that, out, he's, cannot, to, we've, hers, you, did, let's, most, here, these, hasn't, was, there, when's, shan't, doing, at, through, been, over, i've, on, being, same, how, whom, my, after, who, itself, me, them, by, then, couldn't, he, should, few, wasn't, again, while, t..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val stopWords = scala.io.Source.fromFile(\"Data/practice6/stopwords.txt\").getLines().toSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "plainTextToLemmas: (text: String, stopWords: Set[String], pipeline: edu.stanford.nlp.pipeline.StanfordCoreNLP)Seq[String]\n",
       "lemmatized: org.apache.spark.rdd.RDD[Seq[String]] = MapPartitionsRDD[2] at mapPartitions at <console>:58\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def plainTextToLemmas(text: String, stopWords: Set[String], pipeline:StanfordCoreNLP): Seq[String] = {\n",
    "  val doc = new Annotation(text)\n",
    "  pipeline.annotate(doc)\n",
    "  val lemmas = new ArrayBuffer[String]()\n",
    "  val sentences = doc.get(classOf[SentencesAnnotation])\n",
    "  for (sentence <- sentences; token <- sentence.get(classOf[TokensAnnotation])) {\n",
    "    val lemma = token.get(classOf[LemmaAnnotation])\n",
    "    if (lemma.length > 2 && !stopWords.contains(lemma)) {\n",
    "      lemmas += lemma.toLowerCase\n",
    "    }\n",
    "  }\n",
    "  lemmas\n",
    "}\n",
    "\n",
    "val lemmatized = plainText.mapPartitions(strings => {\n",
    "  val props = new Properties()\n",
    "  props.put(\"annotators\", \"tokenize, ssplit, pos, lemma\")\n",
    "  val pipeline = new StanfordCoreNLP(props)\n",
    "  strings.map(string => plainTextToLemmas(string, stopWords, pipeline))\n",
    "})\n",
    "//lemmatized.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "consensus 0.21461164951324463\n",
      "chain 0.2114528864622116\n",
      "base 0.20180371403694153\n",
      "management 0.187967449426651\n",
      "key 0.17615555226802826\n",
      "state 0.17279267311096191\n",
      "distribute 0.16372162103652954\n",
      "perform 0.1537478119134903\n",
      "share 0.14828689396381378\n",
      "peer 0.14213550090789795\n",
      "integrity 0.14040042459964752\n",
      "authentication 0.13889484107494354\n",
      "efficient 0.13813062012195587\n",
      "trust 0.13649748265743256\n",
      "validation 0.1361583024263382\n",
      "software 0.12799331545829773\n",
      "network 0.12795385718345642\n",
      "database 0.10710969567298889\n",
      "access 0.10526232421398163\n",
      "paired 0.10280188918113708\n",
      "user 0.09632086753845215\n",
      "program 0.09115133434534073\n",
      "confidential 0.08733682334423065\n",
      "node 0.08522427082061768\n",
      "frame 0.07410840690135956\n",
      "method 0.06495237350463867\n",
      "wireless 0.061737559735774994\n",
      "ledger 0.0584103949368\n",
      "application 0.05811193585395813\n",
      "multiple 0.05467619374394417\n",
      "content 0.05329390615224838\n",
      "systems 0.04808741435408592\n",
      "media 0.044690463691949844\n",
      "public 0.043381042778491974\n",
      "medium 0.04289785772562027\n",
      "identity 0.039280280470848083\n",
      "secure 0.03729384392499924\n",
      "change 0.03669356554746628\n",
      "exclusive 0.03452027216553688\n",
      "use 0.03026958927512169\n",
      "device 0.030017856508493423\n",
      "datum 0.028619611635804176\n",
      "verification 0.024474285542964935\n",
      "decentralized 0.01978788897395134\n",
      "sharing 0.014054995030164719\n",
      "computer 0.013639288954436779\n",
      "encryption 0.010616421699523926\n",
      "distribution 0.007184893358498812\n",
      "proof 0.007181744556874037\n",
      "track 0.0044992235489189625\n",
      "security 0.0017467016587033868\n",
      "electronic 9.665525867603719E-4\n",
      "communication 8.387346751987934E-4\n",
      "operation -0.0033328281715512276\n",
      "smart -0.007831979542970657\n",
      "scalable -0.015726640820503235\n",
      "support -0.019743580371141434\n",
      "encrypt -0.020015565678477287\n",
      "storage -0.026691585779190063\n",
      "system -0.029557155445218086\n",
      "manage -0.031366560608148575\n",
      "voting -0.04005943611264229\n",
      "provision -0.04092070832848549\n",
      "cryptographic -0.04366027191281319\n",
      "provide -0.044069159775972366\n",
      "computing -0.04614529013633728\n",
      "technique -0.04750329628586769\n",
      "transaction -0.049417320638895035\n",
      "service -0.04962923377752304\n",
      "processing -0.05128452554345131\n",
      "supply -0.057589925825595856\n",
      "receive -0.06436444818973541\n",
      "environment -0.07199085503816605\n",
      "data -0.07826388627290726\n",
      "information -0.07907579839229584\n",
      "block -0.09834643453359604\n",
      "dynamic -0.10316768288612366\n",
      "private -0.1104508489370346\n",
      "apparatus -0.11195452511310577\n",
      "services -0.11334437876939774\n",
      "receiver -0.14284077286720276\n",
      "transmit -0.15031015872955322\n",
      "contract -0.16191983222961426\n",
      "exchange -0.16572615504264832\n",
      "hard -0.16783647239208221\n",
      "protocol -0.17001214623451233\n",
      "control -0.18694937229156494\n",
      "high -0.22176308929920197\n"
     ]
    },
    {
     "ename": "org.apache.hadoop.mapred.FileAlreadyExistsException",
     "evalue": " Output directory file:/root/workspace/myModelPath/metadata already exists",
     "output_type": "error",
     "traceback": [
      "org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/root/workspace/myModelPath/metadata already exists",
      "  at org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)",
      "  at org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:289)",
      "  at org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:71)",
      "  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1096)",
      "  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)",
      "  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:385)",
      "  at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1094)",
      "  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1067)",
      "  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)",
      "  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:385)",
      "  at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1032)",
      "  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:958)",
      "  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)",
      "  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:385)",
      "  at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:957)",
      "  at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1544)",
      "  at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1523)",
      "  at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1523)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:385)",
      "  at org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1523)",
      "  at org.apache.spark.mllib.feature.Word2VecModel$SaveLoadV1_0$.save(Word2Vec.scala:692)",
      "  at org.apache.spark.mllib.feature.Word2VecModel.save(Word2Vec.scala:531)",
      "  ... 43 elided",
      ""
     ]
    }
   ],
   "source": [
    "import org.apache.spark.mllib.feature.{Word2Vec, Word2VecModel}\n",
    "\n",
    "val word2vec = new Word2Vec()\n",
    "val model = word2vec.fit(lemmatized)\n",
    "val synonyms = model.findSynonyms(\"blockchain\", 100)\n",
    "\n",
    "for((synonym, cosineSimilarity) <- synonyms) {\n",
    "  println(s\"$synonym $cosineSimilarity\")\n",
    "}\n",
    "\n",
    "// Save and load model\n",
    "model.save(sc, \"myModelPath\")\n",
    "val sameModel = Word2VecModel.load(sc, \"myModelPath\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
