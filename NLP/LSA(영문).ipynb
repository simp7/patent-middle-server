{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://localhost:4041\n",
       "SparkContext available as 'sc' (version = 2.4.6, master = local[*], app id = local-1606995847684)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.{CountVectorizer, IDF}\n",
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.sql.{DataFrame, Dataset, SparkSession}\n",
       "import scala.collection.JavaConversions._\n",
       "import scala.collection.mutable.ArrayBuffer\n",
       "import java.util.Properties\n",
       "import java.io.ByteArrayInputStream\n",
       "import edu.stanford.nlp.ling.CoreAnnotations.{LemmaAnnotation, SentencesAnnotation, TokensAnnotation}\n",
       "import edu.stanford.nlp.pipeline.{Annotation, StanfordCoreNLP}\n",
       "import spark.implicits._\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.{CountVectorizer, IDF}\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.{DataFrame, Dataset, SparkSession}\n",
    "import scala.collection.JavaConversions._\n",
    "import scala.collection.mutable.ArrayBuffer\n",
    "import java.util.Properties\n",
    "import java.io.ByteArrayInputStream;\n",
    "\n",
    "import edu.stanford.nlp.ling.CoreAnnotations.{LemmaAnnotation, SentencesAnnotation, TokensAnnotation}\n",
    "import edu.stanford.nlp.pipeline.{Annotation, StanfordCoreNLP}\n",
    "\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res0: org.apache.spark.SparkContext = org.apache.spark.SparkContext@38d6685f\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "plainText: org.apache.spark.rdd.RDD[String] = Data/proj/D5.csv MapPartitionsRDD[1] at textFile at <console>:41\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val plainText = sc.textFile(\"Data/proj/D5.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: Array[String] = Array(\"1  . A computer-implemented method of decentralized block chain voting, the method comprising:  retrieving, by a computing device, polling data including a plurality of polling options and an option identifier associated with each polling option;  generating a customized cryptographic currency address for each of the plurality of polling options, based on the corresponding option identifier associated with each polling option; and  transferring a specified amount of cryptographic tokens to the customized cryptographic currency address for a selected one of the plurality of polling options,  wherein the transfer is broadcast to a cryptographic currency network for confirmation and inclusion within a block chain ledger of the cryptographic currency network.  2..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plainText.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stopWords: scala.collection.immutable.Set[String] = Set(down, it's, ourselves, that's, for, method, further, she'll, any, there's, this, haven't, in, ought, myself, have, your, off, once, i'll, are, is, his, why, too, why's, am, than, isn't, didn't, himself, but, you're, below, what, would, methods, i'd, if, you'll, own, they'll, up, we're, they'd, so, our, do, all, him, had, nor, before, it, a, she's, as, hadn't, because, has, she, yours, or, above, yourself, herself, she'd, such, they, each, can't, don't, i, until, that, out, he's, cannot, to, we've, hers, you, did, let's, most, here, these, hasn't, was, there, when's, shan't, doing, at, through, been, over, i've, on, being, same, how, whom, my, after, who, itself, me, them, by, then, couldn't, he, should, few, wasn't, again, while, t..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val stopWords = scala.io.Source.fromFile(\"Data/practice6/stopwords.txt\").getLines().toSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import edu.stanford.nlp.pipeline._\n",
       "import edu.stanford.nlp.ling.CoreAnnotations._\n",
       "plainTextToLemmas: (text: String, stopWords: Set[String], pipeline: edu.stanford.nlp.pipeline.StanfordCoreNLP)Seq[String]\n",
       "lemmatized: org.apache.spark.rdd.RDD[Seq[String]] = MapPartitionsRDD[2] at mapPartitions at <console>:61\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import edu.stanford.nlp.pipeline._\n",
    "import edu.stanford.nlp.ling.CoreAnnotations._\n",
    "\n",
    "def plainTextToLemmas(text: String, stopWords: Set[String], pipeline:StanfordCoreNLP): Seq[String] = {\n",
    "  val doc = new Annotation(text)\n",
    "  pipeline.annotate(doc)\n",
    "  val lemmas = new ArrayBuffer[String]()\n",
    "  val sentences = doc.get(classOf[SentencesAnnotation])\n",
    "  for (sentence <- sentences; token <- sentence.get(classOf[TokensAnnotation])) {\n",
    "    val lemma = token.get(classOf[LemmaAnnotation])\n",
    "    if (lemma.length > 2 && !stopWords.contains(lemma)) {\n",
    "      lemmas += lemma.toLowerCase\n",
    "    }\n",
    "  }\n",
    "  lemmas\n",
    "}\n",
    "\n",
    "val lemmatized = plainText.mapPartitions(strings => {\n",
    "  val props = new Properties()\n",
    "  props.put(\"annotators\", \"tokenize, ssplit, pos, lemma\")\n",
    "  val pipeline = new StanfordCoreNLP(props)\n",
    "  strings.map(string => plainTextToLemmas(string, stopWords, pipeline))\n",
    "})\n",
    "//lemmatized.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "termsDF: org.apache.spark.sql.DataFrame = [terms: array<string>]\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val termsDF = lemmatized.toDF(\"terms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               terms|\n",
      "+--------------------+\n",
      "|[computer, implem...|\n",
      "|[cryptographic, v...|\n",
      "|[system, comprise...|\n",
      "|[comprise, identi...|\n",
      "|[block, generatio...|\n",
      "|[process, vote, p...|\n",
      "|[system, comprise...|\n",
      "|[carry, least, fi...|\n",
      "|[system, comprise...|\n",
      "|[system, secure, ...|\n",
      "|[node, position, ...|\n",
      "|[blockchain, cons...|\n",
      "|[electronic, voti...|\n",
      "|[electronic, voti...|\n",
      "|[electronic, voti...|\n",
      "|[autonomous, vehi...|\n",
      "|[system, securely...|\n",
      "|[comprise, receiv...|\n",
      "|[cryptographic, v...|\n",
      "|[system, comprise...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "termsDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "titleDF: org.apache.spark.sql.DataFrame = [title: string]\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val titleDF = spark.read.\n",
    "    option(\"inferSchema\", true).\n",
    "    option(\"header\", true).\n",
    "    csv(\"Data/proj/T5.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "t1: org.apache.spark.sql.DataFrame = [terms: array<string>, id: bigint]\n",
       "t2: org.apache.spark.sql.DataFrame = [title: string, id: bigint]\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val t1 = termsDF.withColumn(\"id\", monotonically_increasing_id())\n",
    "val t2 = titleDF.withColumn(\"id\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "patentDF: org.apache.spark.sql.DataFrame = [title: string, terms: array<string>]\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val patentDF = t2.join(t1, \"id\").drop(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|               title|               terms|\n",
      "+--------------------+--------------------+\n",
      "|CRYPTOGRAPHIC CUR...|[computer, implem...|\n",
      "|Blockchain-based ...|[cryptographic, v...|\n",
      "|OFF-CHAIN BLOCKCH...|[system, comprise...|\n",
      "|Peer voting on a ...|[comprise, identi...|\n",
      "|BLOCK GENERATION ...|[block, generatio...|\n",
      "|METHOD AND SYSTEM...|[process, vote, p...|\n",
      "|Blockchain-based ...|[system, comprise...|\n",
      "|Methods and appar...|[carry, least, fi...|\n",
      "|SYSTEMS AND METHO...|[system, comprise...|\n",
      "|CRYPTOGRAPHICALLY...|[system, secure, ...|\n",
      "|APPARATUS AND MET...|[node, position, ...|\n",
      "|METHOD, APPARATUS...|[blockchain, cons...|\n",
      "|Electronic voting...|[electronic, voti...|\n",
      "|Electronic voting...|[electronic, voti...|\n",
      "|Electronic voting...|[electronic, voti...|\n",
      "|Secure distribute...|[autonomous, vehi...|\n",
      "|Blockchain fortif...|[system, securely...|\n",
      "|SYSTEMS AND METHO...|[comprise, receiv...|\n",
      "|Blockchain-based ...|[cryptographic, v...|\n",
      "|BLOCKCHAIN COMPUT...|[system, comprise...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "patentDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numTerms: Int = 20000\n",
       "countVectorizer: org.apache.spark.ml.feature.CountVectorizer = cntVec_1481a0fbf72d\n",
       "vocabModel: org.apache.spark.ml.feature.CountVectorizerModel = cntVec_1481a0fbf72d\n",
       "docTermFreqs: org.apache.spark.sql.DataFrame = [title: string, terms: array<string> ... 1 more field]\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val numTerms = 20000\n",
    "val countVectorizer = new CountVectorizer().\n",
    "setInputCol(\"terms\").\n",
    "setOutputCol(\"termFreqs\").\n",
    "setVocabSize(numTerms)\n",
    "val vocabModel = countVectorizer.fit(patentDF)\n",
    "val docTermFreqs = vocabModel.transform(patentDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res4: docTermFreqs.type = [title: string, terms: array<string> ... 1 more field]\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docTermFreqs.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "idf: org.apache.spark.ml.feature.IDF = idf_83efa156a7de\n",
       "idfModel: org.apache.spark.ml.feature.IDFModel = idf_83efa156a7de\n",
       "docTermMatrix: org.apache.spark.sql.DataFrame = [title: string, tfidfVec: vector]\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val idf = new IDF().\n",
    "setInputCol(\"termFreqs\").\n",
    "setOutputCol(\"tfidfVec\")\n",
    "val idfModel = idf.fit(docTermFreqs)\n",
    "val docTermMatrix = idfModel.transform(docTermFreqs).select(\"title\", \"tfidfVec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "termIds: Array[String] = Array(wherein, claim, one, comprise, first, blockchain, second, node, transaction, datum, system, block, plurality, computer, device, least, receive, key, network, include, configure, identifier, processor, determine, base, vote, request, store, associate, information, value, generate, ledger, use, number, record, apparatus, distribute, cryptographic, readable, encrypt, storage, consensus, user, set, message, correspond, server, access, frame, perform, communication, medium, computing, party, voter, security, instruction, transmit, identify, response, execute, memory, voting, control, function, say, media, entity, accord, character, time, select, private, database, trust, program, public, portion, new, token, non-transitory, bit, cause, address, field, compute, ..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val termIds: Array[String] = vocabModel.vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "docIds: scala.collection.immutable.Map[Long,String] = Map(138 -> DOCUMENT TRANSFER PROCESSING FOR BLOCKCHAINS, 234 -> CRYPTOASSET CUSTODIAL SYSTEM WITH DIFFERENT CRYPTOGRAPHIC KEYS CONTROLLING ACCESS TO SEPARATE GROUPS OF PRIVATE KEYS, 0 -> CRYPTOGRAPHIC CURRENCY BLOCK CHAIN BASED VOTING SYSTEM, 88 -> ENABLING ACCESS ACROSS PRIVATE NETWORKS FOR A MANAGED BLOCKCHAIN SERVICE, 352 -> Efficient clearinghouse transactions with trusted and un-trusted entities, 170 -> VOTING SYSTEM AND METHOD, 276 -> Dynamic content distribution protocol for an enterprise environment, 308 -> BYZANTINE AGREEMENT IN OPEN NETWORKS, 120 -> Blockchain Timeclock System, 202 -> Method and system for copying live entities of source blocks identified by source list for selected destination block to selected destination..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val docIds = docTermFreqs.rdd.map(_.getString(0)).\n",
    "zipWithUniqueId().\n",
    "map(_.swap).\n",
    "collect().toMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.mllib.linalg.{Vectors, Vector=>MLLibVector}\n",
       "import org.apache.spark.ml.linalg.{Vector=>MLVector}\n",
       "vecRdd: org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.Vector] = MapPartitionsRDD[67] at map at <console>:51\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.mllib.linalg.{Vectors, Vector => MLLibVector}\n",
    "import org.apache.spark.ml.linalg.{Vector => MLVector}\n",
    "\n",
    "val vecRdd = docTermMatrix.select(\"tfidfVec\").rdd.map { row =>\n",
    "Vectors.fromML(row.getAs[MLVector](\"tfidfVec\"))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSA(latent semantic Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.mllib.linalg.distributed.RowMatrix\n",
       "mat: org.apache.spark.mllib.linalg.distributed.RowMatrix = org.apache.spark.mllib.linalg.distributed.RowMatrix@4a51aa22\n",
       "k: Int = 500\n",
       "svd: org.apache.spark.mllib.linalg.SingularValueDecomposition[org.apache.spark.mllib.linalg.distributed.RowMatrix,org.apache.spark.mllib.linalg.Matrix] =\n",
       "SingularValueDecomposition(org.apache.spark.mllib.linalg.distributed.RowMatrix@628e68e9,[1729.5240106965148,1383.0347146843299,1299.6098644632875,1223.6731226286024,1122.4545355555204,1099.2715388184295,1002.2768406200446,964.7291645012558,929.9797759987968,927.2915659974697,897.7475721775903,879.4837146115256,849.5819026996015,838.4496239858136,826.8733146926664,796.6586015193386,791.6847810376802,757.6977146003904,738.0340386036961,730.448481420..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.mllib.linalg.distributed.RowMatrix\n",
    "\n",
    "vecRdd.cache()\n",
    "val mat = new RowMatrix(vecRdd)\n",
    "val k = 500\n",
    "val svd = mat.computeSVD(k, computeU=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.mllib.linalg.{Matrix, SingularValueDecomposition}\n",
       "import org.apache.spark.mllib.linalg.distributed.RowMatrix\n",
       "topTermsInTopConcepts: (svd: org.apache.spark.mllib.linalg.SingularValueDecomposition[org.apache.spark.mllib.linalg.distributed.RowMatrix,org.apache.spark.mllib.linalg.Matrix], numConcepts: Int, numTerms: Int, termIds: Array[String])Seq[Seq[(String, Double)]]\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.mllib.linalg.{Matrix, SingularValueDecomposition}\n",
    "import org.apache.spark.mllib.linalg.distributed.RowMatrix\n",
    "\n",
    "def topTermsInTopConcepts(\n",
    "svd: SingularValueDecomposition[RowMatrix, Matrix],\n",
    "numConcepts: Int,\n",
    "numTerms: Int, termIds: Array[String])\n",
    ":Seq[Seq[(String, Double)]] = {\n",
    "    val v = svd.V\n",
    "    val topTerms = new ArrayBuffer[Seq[(String, Double)]]()\n",
    "    val arr = v.toArray\n",
    "    for (i <- 0 until numConcepts) {\n",
    "        val offs = i * v.numRows\n",
    "        val termWeights = arr.slice(offs, offs + v.numRows).zipWithIndex\n",
    "        val sorted = termWeights.sortBy(-_._1)\n",
    "        topTerms += sorted.take(numTerms).map {\n",
    "            case (score, id) => (termIds(id), score)\n",
    "        }\n",
    "    }\n",
    "    topTerms\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topDocsInTopConcepts: (svd: org.apache.spark.mllib.linalg.SingularValueDecomposition[org.apache.spark.mllib.linalg.distributed.RowMatrix,org.apache.spark.mllib.linalg.Matrix], numConcepts: Int, numDocs: Int, docIds: Map[Long,String])Seq[Seq[(String, Double)]]\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def topDocsInTopConcepts(\n",
    "svd: SingularValueDecomposition[RowMatrix, Matrix],\n",
    "numConcepts: Int, numDocs: Int, docIds: Map[Long, String])\n",
    ": Seq[Seq[(String, Double)]] = {\n",
    "    val u = svd.U\n",
    "    val topDocs = new ArrayBuffer[Seq[(String, Double)]]()\n",
    "for( i <- 0 until numConcepts) {\n",
    "    val docWeights = u.rows.map(_.toArray(i)).zipWithUniqueId()\n",
    "    topDocs += docWeights.top(numDocs).map {\n",
    "        case (score, id) => (docIds(id), score)\n",
    "    }\n",
    "}\n",
    "topDocs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concept terms: elapsed, movable, driveable, trajectory, simply\n",
      "Concept docs: Distributive networks of groups of moveable autonomous devices. Method for Smart Contract Data Input through a Proof-of-Work Consensus Mechanism. Massively Scalable Blockchain Ledger. Systems and Methods for Dynamic Cypher Key Management. Flexible Blockchain Smart-Contract Deployment\n",
      "\n",
      "Concept terms: character, directory, name, decode, recite\n",
      "Concept docs: Exclusive encryption. Exclusive encryption. Exclusive encryption. Exclusive encryption. Method for Smart Contract Data Input through a Proof-of-Work Consensus Mechanism\n",
      "\n",
      "Concept terms: frame, field, control, party, crosslink\n",
      "Concept docs: Methods and apparatus for communicating high efficiency control information. Communication apparatus, communication system, and communication control program. Communication apparatus, communication system and communication control program. Efficient clearinghouse transactions with trusted and un-trusted entities. Interlocked blockchains to increase blockchain security\n",
      "\n",
      "Concept terms: field, frame, control, grab, physical\n",
      "Concept docs: Methods and apparatus for communicating high efficiency control information. Communication apparatus, communication system, and communication control program. Communication apparatus, communication system and communication control program. Method and system for copying live entities of source blocks identified by source list for selected destination block to selected destination block of memory heap. Method and apparatus for transmitting and receiving frame supporting short MAC header in wireless LAN system\n",
      "\n",
      "Concept terms: hsm, security, api, function, call\n",
      "Concept docs: Hardware blockchain acceleration. Hardware blockchain acceleration. Real-time communication security for automation networks. Control information for public switched telephone network (PSTN) using blockchain system. Blockchain fortified aircraft communications addressing and reporting system (ACARS) communication\n",
      "\n",
      "Concept terms: crosslink, digest, transaction, second, last\n",
      "Concept docs: Interlocked blockchains to increase blockchain security. Blockchain-based social media history maps. Blockchain fortified aircraft communications addressing and reporting system (ACARS) communication. Method and system for copying live entities of source blocks identified by source list for selected destination block to selected destination block of memory heap. Linear view-change BFT with optimistic responsiveness\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "topConceptTerms: Seq[Seq[(String, Double)]] = ArrayBuffer(ArraySeq((elapsed,-7.35774369583185E-7), (movable,-7.357743695857871E-7), (driveable,-7.357743695883892E-7), (trajectory,-7.357743695974965E-7), (simply,-1.147520937130192E-6)), ArraySeq((character,0.04621677006392272), (directory,0.033459889038937346), (name,0.027513886217600397), (decode,0.02596357621401845), (recite,0.021620478949759908)), ArraySeq((frame,0.4557077012816694), (field,0.44240159831639514), (control,0.18860630226458705), (party,0.1759912845645021), (crosslink,0.1548386631537958)), ArraySeq((field,0.3369040896776928), (frame,0.31608290057553606), (control,0.09837342321626272), (grab,0.08798058875313144), (physical,0.07218577996770213)), ArraySeq((hsm,0.4921007526197303), (security,0.46920133841553135), (api,0.2924..."
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val topConceptTerms = topTermsInTopConcepts(svd, 6, 5, termIds)\n",
    "val topConceptDocs = topDocsInTopConcepts(svd, 6, 5, docIds)\n",
    "for((terms, docs) <- topConceptTerms.zip(topConceptDocs)) {\n",
    "    println(\"Concept terms: \" + terms.map(_._1).mkString(\", \"))\n",
    "    println(\"Concept docs: \" + docs.map(_._1).mkString(\". \"))\n",
    "    println()\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
